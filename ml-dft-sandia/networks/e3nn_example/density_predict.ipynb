{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import random\n",
    "from functools import partial\n",
    "\n",
    "from e3nn.kernel import Kernel\n",
    "from e3nn.point.operations import Convolution\n",
    "from e3nn.non_linearities import GatedBlock\n",
    "from e3nn.non_linearities import rescaled_act\n",
    "from e3nn.non_linearities.rescaled_act import relu, sigmoid\n",
    "from e3nn.radial import CosineBasisModel\n",
    "from e3nn.radial import GaussianRadialModel\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load density data\n",
    "picklename = \"./density_data/dimer_data.pckl\"\n",
    "with open(picklename, 'rb') as f:\n",
    "    dataset_coeffs, dataset_onehot, dataset_geom, dataset_typemap, Rs_out_list, coeff_by_type = pickle.load(f)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks import *\n",
    "\n",
    "## set arguments to network\n",
    "maxradius = 3.0\n",
    "numbasis = 20\n",
    "radiallayers = 3\n",
    "radialbasis = \"Gaussian\"\n",
    "## set Rs_in based on onehot vector\n",
    "Rs_in = [(len(dataset_typemap[0]),0)]\n",
    "\n",
    "print(\"Rs_in:\",Rs_in)\n",
    "print(\"\\nOxygen Rs_out:\",Rs_out_list[0])\n",
    "print(\"Hydrogen Rs_out:\",Rs_out_list[1])\n",
    "\n",
    "mydict = {\"Rs_in\":Rs_in, \"Rs_out_list\":(Rs_out_list), \"max_radius\":maxradius,\n",
    "            \"number_of_basis\":numbasis, \"radial_layers\":radiallayers, \n",
    "            \"basistype\":radialbasis}\n",
    "\n",
    "#net = MixerNetwork(**mydict)\n",
    "net = SplitNetwork(**mydict)\n",
    "\n",
    "print(net)\n",
    "\n",
    "\n",
    "#net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up training\n",
    "\n",
    "net.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-2)\n",
    "optimizer.zero_grad()\n",
    "loss_fn = torch.nn.modules.loss.MSELoss()\n",
    "\n",
    "max_steps = 2000\n",
    "minibatch_size = 16\n",
    "\n",
    "print (device)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_minibatch = 0\n",
    "for step in range(max_steps):\n",
    "    i = random.randint(0, len(dataset_geom) - 3001)\n",
    "\n",
    "    onehot = dataset_onehot[i]\n",
    "    points = dataset_geom[i]\n",
    "    atom_type_map = dataset_typemap[i]\n",
    "    coeffs = dataset_coeffs[i]\n",
    "\n",
    "    outputO, outputH = net(onehot.to(device),points.to(device),atom_type_map)\n",
    "    outputO = torch.flatten(outputO)\n",
    "    outputH = torch.flatten(outputH)\n",
    "    output = torch.cat((outputO,outputH),0).view(1,1,-1)\n",
    "\n",
    "    loss = loss_fn(output, coeffs)\n",
    "    step_loss = loss.item()\n",
    "    loss.backward()\n",
    "    loss_minibatch += step_loss\n",
    "\n",
    "    if (step+1)%minibatch_size == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss_minibatch = 0\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print('\\nStep {0}, Loss {1}'.format(step, step_loss))\n",
    "        j = random.randint(3000, len(dataset_geom) - 1)\n",
    "\n",
    "        onehot = dataset_onehot[j]\n",
    "        points = dataset_geom[j]*3\n",
    "        atom_type_map = dataset_typemap[j]\n",
    "        coeffs = dataset_coeffs[j]\n",
    "\n",
    "        outputO, outputH = net(onehot.to(device),points.to(device),atom_type_map)\n",
    "        outputO = torch.flatten(outputO)\n",
    "        outputH = torch.flatten(outputH)\n",
    "        output = torch.cat((outputO,outputH),0).view(1,1,-1)\n",
    "\n",
    "        loss = loss_fn(output.to(device), coeffs.to(device))\n",
    "        print('\\nTest Loss {0}'.format(loss.item()))\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from density_analysis_utils import *\n",
    "\n",
    "testnumelectrons(net,device,2,\"./density_data/a2.gbs\",dataset_onehot,dataset_geom,dataset_typemap,coeff_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from density_analysis_utils import *\n",
    "from e3nn.rs import dim, mul_dim\n",
    "\n",
    "## define Gaussian Type Orbital basis functions\n",
    "basis = lambda r, alpha, norm : norm * torch.exp(- alpha * r.unsqueeze(-1) **2)\n",
    "\n",
    "## get exponent alphas\n",
    "alphaO, alphaH = get_exponents('./density_data/a2.gbs')\n",
    "\n",
    "## get normalization constants\n",
    "normO, normH = parse_whole_normfile('./density_data/a2_norm.dat')\n",
    "normO = torch.FloatTensor(normO)\n",
    "normH = torch.FloatTensor(normH)\n",
    "\n",
    "## get spherical harmonic normalization constants\n",
    "Rs_out_O = Rs_out_list[0]\n",
    "Rs_out_H = Rs_out_list[1]\n",
    "sph_normsO, sph_normsH = get_spherical_harmonic_norms(Rs_out_O,Rs_out_H)\n",
    "\n",
    "basis_on_r_O = partial(basis, alpha=alphaO, norm=normO)\n",
    "basis_on_r_H = partial(basis, alpha=alphaH, norm=normH)\n",
    "\n",
    "assert mul_dim(Rs_out_O) == normO.shape[0]\n",
    "assert mul_dim(Rs_out_H) == normH.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a random structure to test\n",
    "\n",
    "dimer_num = 4321\n",
    "onehot = dataset_onehot[dimer_num]\n",
    "points = dataset_geom[dimer_num]\n",
    "atom_type_map = dataset_typemap[dimer_num]\n",
    "outputO, outputH = net(onehot.to(device),points.to(device),atom_type_map)\n",
    "\n",
    "outputO = outputO.data.cpu().numpy()\n",
    "outputH = outputH.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spherical import plot_data_on_grid\n",
    "import e3nn.o3 as o3\n",
    "\n",
    "## get the functions\n",
    "f_list = []\n",
    "# loop over types\n",
    "for i, type in enumerate(atom_type_map):\n",
    "    # loop over atoms\n",
    "    for count, atom in enumerate(type):\n",
    "        tot_f = 0\n",
    "        center = points.data.squeeze().numpy()[atom]\n",
    "        # oxygens\n",
    "        if i == 0:\n",
    "            #vsf = VisualizeSphericalFunction(basis_on_r_O, Rs_out_O, o3.spherical_harmonics_xyz)\n",
    "            r, f = plot_data_on_grid(5.0, basis_on_r_O, Rs_out_O,\n",
    "                                         n=20, center=center)\n",
    "            for j, val in enumerate(outputO.squeeze()[count]):\n",
    "                c = val\n",
    "                norm = sph_normsO[j]\n",
    "                # sum up contributions from every basis function\n",
    "                tot_f += c*f[:,j]/norm\n",
    "        # hydrogens\n",
    "        if i == 1:\n",
    "            #vsf = VisualizeSphericalFunction(basis_on_r_H, Rs_out_H, o3.spherical_harmonics_xyz)\n",
    "            r, f = plot_data_on_grid(5.0, basis_on_r_H, Rs_out_H,\n",
    "                                         n=20, center=center)\n",
    "            for j, val in enumerate(outputH.squeeze()[count]):\n",
    "                c = val\n",
    "                norm = sph_normsH[j]\n",
    "                # sum up contributions from every basis function\n",
    "                tot_f += c*f[:,j]/norm\n",
    "\n",
    "        f_list.append(tot_f)\n",
    "\n",
    "all_atom_f = sum(f_list)\n",
    "print(all_atom_f.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "plot_max = float(all_atom_f.max())\n",
    "\n",
    "fig = go.Figure(data=go.Volume(\n",
    "    x=r[:,0],\n",
    "    y=r[:,1],\n",
    "    z=r[:,2],\n",
    "    #value=c * f[:, i],\n",
    "    value=all_atom_f,\n",
    "    isomin=-0.005*plot_max,\n",
    "    isomax=0.005*plot_max,\n",
    "    #isomin=-0.03,\n",
    "    #isomax=0.03,\n",
    "    opacity=0.3, # needs to be small to see through all surfaces\n",
    "    opacityscale=\"uniform\",\n",
    "    surface_count=50, # needs to be a large number for good volume rendering\n",
    "    colorscale='RdBu'))\n",
    "    \n",
    "xs = points.data.squeeze().numpy()[:,0]\n",
    "ys = points.data.squeeze().numpy()[:,1]\n",
    "zs = points.data.squeeze().numpy()[:,2]\n",
    "fig.add_scatter3d(x=xs,y=ys,z=zs,mode='markers',marker=dict(size=12,color='Black',opacity=1.0))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e3nn_dos",
   "language": "python",
   "name": "e3nn_dos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}